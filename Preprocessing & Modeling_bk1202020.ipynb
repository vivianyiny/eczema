{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install ftfy\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Imports\n",
    "import nltk\n",
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "import re\n",
    "import ftfy\n",
    "from IPython.display                 import display_html\n",
    "from IPython.core.display            import display, HTML\n",
    "\n",
    "# Proprocessing, Modeling, & Evaluation\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer \n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics                 import accuracy_score, recall_score, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.svm                     import SVC\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from xgboost                         import XGBClassifier\n",
    "\n",
    "# Custom Modules\n",
    "import graphs\n",
    "import metrics\n",
    "\n",
    "\n",
    "# Notebook settings & styles\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotated_Sample = pd.read_csv('./Reddit Data/All/Reddit Eczema_All_20201201_Sample_Annotated.csv', encoding = \"iso-8859-1\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ying.Ying_Zenbook\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ying.Ying_Zenbook\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading the default stopwords\n",
    "\n",
    "nltk.download(\"stopwords\");\n",
    "\n",
    "# Adding our stopwords to the English set\n",
    "\n",
    "#new_stopwords = [\"like\", \"just\", \"make\", \"cook\",\"use\", \"chicken\", \"recipe\", \"sauce\"]\n",
    "\n",
    "stopwords     = stopwords.words('english')\n",
    "\n",
    "#stopwords.extend(new_stopwords)\n",
    "\n",
    "# Instantiating the lemmatizier and tokenizer\n",
    "# The tokenizer will only keep text\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer  = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(dataframe):\n",
    "   \n",
    "    dataframe ['body'] = dataframe ['body'].map(ftfy.fix_encoding)\n",
    "    \n",
    "    if 'Annotation' in dataframe :\n",
    "        dataframe[\"label\"] = dataframe[\"Annotation\"].apply(lambda x: 1 if x == \"R\" else 0)\n",
    "    \n",
    "    #Annotated_Sample.drop(columns=['Annotation'],inplace=True)\n",
    "    dataframe.rename(columns={'Author':'author'},inplace=True)\n",
    "    \n",
    "    # Setting up the lemmatizer\n",
    "\n",
    "    lemmatized_posts = []\n",
    "\n",
    "    for post in dataframe[\"body\"]:\n",
    "        tokens = tokenizer.tokenize(post)\n",
    "        post   = [lemmatizer.lemmatize(post) for post in tokens]\n",
    "        lemmatized_posts.append(\" \".join(post))\n",
    "\n",
    "    # Appending the lemmatized posts to the dataframe\n",
    "\n",
    "    dataframe[\"lemmatized_text\"] = lemmatized_posts\n",
    "\n",
    "    #remove URL\n",
    "    dataframe[\"lemmatized_text\"] = dataframe[\"lemmatized_text\"].str.replace(\"http\\S+\", \"\")\n",
    "    \n",
    "    #lower case\n",
    "    dataframe[\"lemmatized_text\"] = dataframe[\"lemmatized_text\"].str.lower()\n",
    "    \n",
    "     #remove none letters\n",
    "    dataframe[\"lemmatized_text\"] =  dataframe[\"lemmatized_text\"].apply(lambda x: re.sub(r'[^a-z]',' ', x))\n",
    "\n",
    "\n",
    "    # Checking the head of the dataframe\n",
    "    #dataframe.head()\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hanniballbearings</td>\n",
       "      <td>It's just one of those things. I hate that it ...</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>it s just one of those thing i hate that it do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleverleper</td>\n",
       "      <td>I feel you. Ive thought the same during awful ...</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>i feel you ive thought the same during awful f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sd_red_lobster</td>\n",
       "      <td>you can also try asking on /r/eczeMABs there h...</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>you can also try asking on r eczemabs there ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rumanaaa</td>\n",
       "      <td>I've never heard of this. Thank you for your i...</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>i ve never heard of this thank you for your in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>touchyfeelies</td>\n",
       "      <td>Thanks!</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                               body  \\\n",
       "0  Hanniballbearings  It's just one of those things. I hate that it ...   \n",
       "1        cleverleper  I feel you. Ive thought the same during awful ...   \n",
       "2     sd_red_lobster  you can also try asking on /r/eczeMABs there h...   \n",
       "3           Rumanaaa  I've never heard of this. Thank you for your i...   \n",
       "4      touchyfeelies                                            Thanks!   \n",
       "\n",
       "  Annotation  label                                    lemmatized_text  \n",
       "0          R      1  it s just one of those thing i hate that it do...  \n",
       "1          I      0  i feel you ive thought the same during awful f...  \n",
       "2          R      1  you can also try asking on r eczemabs there ha...  \n",
       "3          I      0  i ve never heard of this thank you for your in...  \n",
       "4          I      0                                             thanks  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Annotated_Sample = cleaning_data(Annotated_Sample)\n",
    "Annotated_Sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Generating a list of text lengths\n",
    "\n",
    "lengths = [len(text) for text in Annotated_Sample[\"body\"]]\n",
    "\n",
    "# Plotting the text lengths\n",
    "\n",
    "plt.figure(figsize = (16,6), facecolor = \"white\")\n",
    "sns.distplot(lengths, kde = False, bins = 100, color = \"black\")\n",
    "plt.axvline(np.mean(lengths), color = \"red\")\n",
    "plt.title(\"Distribution Of Text Length\", size = 18)\n",
    "plt.xlabel(\"Words\", size = 16)\n",
    "plt.ylabel(\"Frequency\", size = 16)\n",
    "plt.xticks(np.arange(0,23500,1500), size = 14)\n",
    "plt.yticks(size = 14);\n",
    "\n",
    "# The red line marks the mean length\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Saving the vectorized dfs to a new dataframe\n",
    "vec = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# Fit-transforming the vectorizer\n",
    "vec_sample     = vec.fit_transform(Annotated_Sample[\"body\"])\n",
    "\n",
    "sample_vectorized     = pd.DataFrame(vec_sample.toarray(), columns = vec.get_feature_names())\n",
    "sample_vectorized.sum().sort_values(ascending=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eczema            185\n",
       "skin              153\n",
       "wa                130\n",
       "like              117\n",
       "use               102\n",
       "                 ... \n",
       "nail looked         1\n",
       "nail done           1\n",
       "nail anyway         1\n",
       "nail allergy        1\n",
       "zyrtec tylenol      1\n",
       "Length: 14232, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_lem = CountVectorizer(ngram_range =(1,2),stop_words = stopwords)\n",
    "\n",
    "vec_sample_lem     = vec_lem.fit_transform(Annotated_Sample[\"lemmatized_text\"])\n",
    "\n",
    "sample_vectorized_lem     = pd.DataFrame(vec_sample_lem.toarray(), columns = vec_lem.get_feature_names())\n",
    "\n",
    "sample_vectorized_lem.sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. \n",
    "\n",
    "\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thing hate        0.187648\n",
       "nothing person    0.187648\n",
       "sometimes keep    0.187648\n",
       "ha nothing        0.187648\n",
       "fault             0.187648\n",
       "                    ...   \n",
       "provide hope      0.000000\n",
       "provide           0.000000\n",
       "proud             0.000000\n",
       "protopic work     0.000000\n",
       "aaf               0.000000\n",
       "Name: 0, Length: 14232, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_lem = TfidfVectorizer(ngram_range =(1,2),stop_words = stopwords)\n",
    "\n",
    "tvec_sample_lem     = tvec_lem.fit_transform(Annotated_Sample[\"lemmatized_text\"])\n",
    "\n",
    "tsample_vectorized_lem     = pd.DataFrame(tvec_sample_lem.toarray(),columns = tvec_lem.get_feature_names())\n",
    "\n",
    "tsample_vectorized_lem.T[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "__Create Test and Training data split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = Annotated_Sample[\"lemmatized_text\"]\n",
    "y = Annotated_Sample[\"label\"]\n",
    "\n",
    "# The random state ensures reproducability\n",
    "# The stratify argument preserves the distribution of classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Logistic Regression__\n",
    "\n",
    "The logistic regression is very similar to the linear regression, but it uses a logit function to bend the line so that it can predict either 0 or 1.\n",
    "\n",
    "The gridsearch will be searching hyperparameters for the vectorizers, not the logistic regression.\n",
    "\n",
    "__Count Vectorizer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "cvec_lr_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting the CVEC hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [None], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [None]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_lr_gs = GridSearchCV(cvec_lr_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 10)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "cvec_lr_gs.fit(X_train, y_train);\n",
    "\n",
    "# The futurewarning can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_lr_train_preds = cvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "cvec_lr_preds       = cvec_lr_gs.predict(X_test)\n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "cvec_lr_probas      = cvec_lr_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.997333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.996795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.990565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.998397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.997333\n",
       "Sensitivity           0.996795\n",
       "Specificity           0.984375\n",
       "AUROC                 0.990565\n",
       "Matthews Corr. Coef.  0.998397"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, cvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.942308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.508376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.733059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.872000\n",
       "Sensitivity           0.942308\n",
       "Specificity           0.647059\n",
       "AUROC                 0.508376\n",
       "Matthews Corr. Coef.  0.733059"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, cvec_lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>6</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                   11                 10\n",
       "Actual Related                      6                 98"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   cvec_lr_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TFIDF Vectorization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "tvec_lr_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"log_reg\", LogisticRegression())])\n",
    "\n",
    "# Setting TFIDF hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [None], \n",
    "                    \"tvec__ngram_range\" : [(1,2)], \n",
    "                    \"tvec__stop_words\"  : [stopwords]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_lr_gs = GridSearchCV(tvec_lr_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 10)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "tvec_lr_gs.fit(X_train, y_train);\n",
    "\n",
    "# The warning is a futurewarning and can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_lr_train_preds = tvec_lr_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "tvec_lr_preds       = tvec_lr_gs.predict(X_test) \n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "tvec_lr_probas     = tvec_lr_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training metrics\n",
    "\n",
    "# metrics.binary_classification_summary(y_train, tvec_lr_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.283772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.848000\n",
       "Sensitivity           1.000000\n",
       "Specificity           1.000000\n",
       "AUROC                 0.283772\n",
       "Matthews Corr. Coef.  0.547619"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, tvec_lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    2                 19\n",
       "Actual Related                      0                104"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   tvec_lr_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "A support vector machine (in this case a classifier) is at its core a linear model. However, instead of running like a logistic regression, it seeks to linearly separate the data. To do that, it uses a kernel to raise the data into n-dimensional space. It then uses a line, plane (3-dimensional line), or hyperplane (greater than 3-dimensions) to delineate the data\n",
    "\n",
    "__Count Vectorizer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "cvec_svc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting CVEC and pipe hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [None], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [stopwords],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"linear\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_svc_gs = GridSearchCV(cvec_svc_pipe, \n",
    "                           param_grid = cvec_pipe_params, \n",
    "                           cv         = 10)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "cvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating training predictions\n",
    "\n",
    "cvec_svc_train_preds = cvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "cvec_svc_preds       = cvec_svc_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "Accuracy                1.0\n",
       "Sensitivity             1.0\n",
       "Specificity             1.0\n",
       "AUROC                   1.0\n",
       "Matthews Corr. Coef.    1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, cvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.517241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.513420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.789835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.840000\n",
       "Sensitivity           0.865385\n",
       "Specificity           0.517241\n",
       "AUROC                 0.513420\n",
       "Matthews Corr. Coef.  0.789835"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, cvec_svc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>14</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                   15                  6\n",
       "Actual Related                     14                 90"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   cvec_svc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF Vectorizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "tvec_svc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF and pipe hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [None], \n",
    "                    \"tvec__ngram_range\" : [(1,2)], \n",
    "                    \"tvec__stop_words\"  : [stopwords],\n",
    "                    \"svc__C\"            : [1.0],\n",
    "                    \"svc__kernel\"       : [\"rbf\"],\n",
    "                    \"svc__gamma\"        : [\"auto\"]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_svc_gs = GridSearchCV(tvec_svc_pipe, \n",
    "                           param_grid = tvec_pipe_params, \n",
    "                           cv         = 10)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "tvec_svc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_svc_train_preds = tvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "tvec_svc_preds       = tvec_svc_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "Accuracy              0.832\n",
       "Sensitivity           1.000\n",
       "Specificity             NaN\n",
       "AUROC                 0.000\n",
       "Matthews Corr. Coef.  0.500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, tvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "# metrics.binary_classification_summary(y_test, tvec_svc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    0                 21\n",
       "Actual Related                      0                104"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   tvec_svc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A random forest classifier is a decision tree-based classification method. However, it has advantages over other tree-based models. Firstly, it bootstraps the dataframe to have a random subset of the data, but it also takes a random subset of the features. Having two levels of randomness in the model reduces the likelihood of the model being overfit on training data but it also allows the model to be less prone to variance caused by many features.\n",
    "\n",
    "__Count Vectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "\n",
    "cvec_rf_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting CVEC and pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [None], \n",
    "                    \"cvec__ngram_range\"    : [(1,2)], \n",
    "                    \"cvec__stop_words\"     : [stopwords],\n",
    "                    \"rf__n_estimators\"     : [72],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [20]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_rf_gs = GridSearchCV(cvec_rf_pipe, \n",
    "                          param_grid = cvec_pipe_params, \n",
    "                          cv         = 10,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "cvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "cvec_rf_train_preds = cvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "cvec_rf_preds       = cvec_rf_gs.predict(X_test) \n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "cvec_rf_probas      = cvec_rf_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "Accuracy              0.832\n",
       "Sensitivity           1.000\n",
       "Specificity             NaN\n",
       "AUROC                 0.000\n",
       "Matthews Corr. Coef.  0.500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, cvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics\n",
    "\n",
    "# metrics.binary_classification_summary(y_test, cvec_rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    0                 21\n",
       "Actual Related                      0                104"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   cvec_rf_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TFIDF Vectorizer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the pipeline\n",
    "\n",
    "tvec_rf_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"rf\", RandomForestClassifier(random_state = 42))])\n",
    "\n",
    "# Setting the TVEC and pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [None], \n",
    "                    \"tvec__ngram_range\"    : [(1,2)], \n",
    "                    \"tvec__stop_words\"     : [stopwords],\n",
    "                    \"rf__n_estimators\"     : [30],\n",
    "                    \"rf__min_samples_split\": [6],\n",
    "                    \"rf__min_samples_leaf\" : [2],\n",
    "                    \"rf__max_depth\"        : [12]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_rf_gs = GridSearchCV(tvec_rf_pipe, \n",
    "                          param_grid = tvec_pipe_params, \n",
    "                          cv         = 5,\n",
    "                          n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_rf_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_rf_train_preds = tvec_rf_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "tvec_rf_preds       = tvec_rf_gs.predict(X_test) \n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "tvec_rf_probas      = tvec_rf_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "Accuracy              0.832\n",
       "Sensitivity           1.000\n",
       "Specificity             NaN\n",
       "AUROC                 0.000\n",
       "Matthews Corr. Coef.  0.500"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, tvec_rf_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score\n",
       "Accuracy              0.832\n",
       "Sensitivity           1.000\n",
       "Specificity             NaN\n",
       "AUROC                 0.000\n",
       "Matthews Corr. Coef.  0.500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, tvec_rf_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    0                 21\n",
       "Actual Related                      0                104"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   tvec_rf_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier\n",
    "\n",
    "XGBoost is a tree-based boosting model that iteratively fits tree models on the errors of the previous model and uses gradient descent to help minimize the loss function. Furthermore, the XGBoost is much more computationally efficient and can be parallelized unlike other boosting models.\n",
    "\n",
    "__Count Vectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:04:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "cvec_xgbc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  seed                  = 42,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting CVEC and pipeline hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\"   : [None], \n",
    "                    \"cvec__ngram_range\"    : [(1,2)], \n",
    "                    \"cvec__stop_words\"     : [stopwords],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.04],\n",
    "                    \"xgbc__n_estimators\"   : [175],\n",
    "                    \"xgbc__gamma\"          : [3.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_xgbc_gs = GridSearchCV(cvec_xgbc_pipe, \n",
    "                            param_grid = cvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "cvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating training predictions\n",
    "\n",
    "cvec_xgbc_train_preds = cvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "cvec_xgbc_preds       = cvec_xgbc_gs.predict(X_test) \n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "cvec_xgbc_probas      = cvec_xgbc_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.328563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.563492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.853333\n",
       "Sensitivity           1.000000\n",
       "Specificity           1.000000\n",
       "AUROC                 0.328563\n",
       "Matthews Corr. Coef.  0.563492"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, cvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.235864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.561813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.840000\n",
       "Sensitivity           0.980769\n",
       "Specificity           0.600000\n",
       "AUROC                 0.235864\n",
       "Matthews Corr. Coef.  0.561813"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, cvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    3                 18\n",
       "Actual Related                      2                102"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   cvec_xgbc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TFIDF Vectorizer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:05:12] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { early_stopping_rounds } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating the pipeline\n",
    "# The model's best parameters are shown\n",
    "\n",
    "tvec_xgbc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                           (\"xgbc\", XGBClassifier(n_jobs                = 6,\n",
    "                                                  seed                  = 42,\n",
    "                                                  early_stopping_rounds = 10))])\n",
    "\n",
    "# Setting the TFIDF and pipeline hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\"   : [525], \n",
    "                    \"tvec__ngram_range\"    : [(1,2)], \n",
    "                    \"tvec__stop_words\"     : [stopwords],\n",
    "                    \"xgbc__max_depth\"      : [3],\n",
    "                    \"xgbc__learning_rate\"  : [0.25],\n",
    "                    \"xgbc__n_estimators\"   : [139],\n",
    "                    \"xgbc__gamma\"          : [1.0]}\n",
    "\n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_xgbc_gs = GridSearchCV(tvec_xgbc_pipe, \n",
    "                            param_grid = tvec_pipe_params, \n",
    "                            cv         = 5,\n",
    "                            n_jobs     = 6)\n",
    "\n",
    "# Fitting the model to the testing data\n",
    "\n",
    "tvec_xgbc_gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_xgbc_train_preds = tvec_xgbc_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "tvec_xgbc_preds       = tvec_xgbc_gs.predict(X_test) \n",
    "\n",
    "# Generating test probabilities\n",
    "\n",
    "tvec_xgbc_probas      = tvec_xgbc_gs.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.882667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.996795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.952381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.510997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.657128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.882667\n",
       "Sensitivity           0.996795\n",
       "Specificity           0.952381\n",
       "AUROC                 0.510997\n",
       "Matthews Corr. Coef.  0.657128"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, tvec_xgbc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.585623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.848000\n",
       "Sensitivity           0.980769\n",
       "Specificity           0.666667\n",
       "AUROC                 0.299500\n",
       "Matthews Corr. Coef.  0.585623"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_test, tvec_xgbc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                    4                 17\n",
       "Actual Related                      2                102"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   tvec_xgbc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows for dataframes to be displayed side-by-side\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace('table', 'table style=\"display:inline\"'), raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "# Count vectorizer metrics\n",
    "\n",
    "cvec_accuracy          = [accuracy_score(y_test, cvec_lr_preds), \n",
    "                          accuracy_score(y_test, cvec_svc_preds),\n",
    "                          accuracy_score(y_test, cvec_rf_preds), \n",
    "                          accuracy_score(y_test, cvec_xgbc_preds)]\n",
    "\n",
    "cvec_specificity       = [metrics.specificity(y_test, cvec_lr_preds), \n",
    "                          metrics.specificity(y_test, cvec_svc_preds),\n",
    "                          metrics.specificity(y_test, cvec_rf_preds), \n",
    "                          metrics.specificity(y_test, cvec_xgbc_preds)]\n",
    "\n",
    "cvec_sensitivity       = [recall_score(y_test, cvec_lr_preds), \n",
    "                          recall_score(y_test, cvec_svc_preds),\n",
    "                          recall_score(y_test, cvec_rf_preds), \n",
    "                          recall_score(y_test, cvec_xgbc_preds)]\n",
    "\n",
    "cvec_rocauc_score      = [roc_auc_score(y_test, cvec_lr_preds),\n",
    "                          roc_auc_score(y_test, cvec_svc_preds),\n",
    "                          roc_auc_score(y_test, cvec_rf_preds),\n",
    "                          roc_auc_score(y_test, cvec_xgbc_preds)]\n",
    "\n",
    "cvec_matthews_corrcoef = [matthews_corrcoef(y_test, cvec_lr_preds),\n",
    "                         matthews_corrcoef(y_test, cvec_svc_preds),\n",
    "                         matthews_corrcoef(y_test, cvec_rf_preds),\n",
    "                         matthews_corrcoef(y_test, cvec_xgbc_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ying.Ying_Zenbook\\Documents\\GitHub\\GWU\\Eczema\\metrics.py:107: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  specificity = cm[0,0] / (cm[0,0] + cm[1,0])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TFIDF vectorizer metrics\n",
    "\n",
    "tvec_accuracy          = [accuracy_score(y_test, tvec_lr_preds), \n",
    "                          accuracy_score(y_test, tvec_svc_preds),\n",
    "                          accuracy_score(y_test, tvec_rf_preds), \n",
    "                          accuracy_score(y_test, tvec_xgbc_preds)]\n",
    "\n",
    "tvec_specificity       = [metrics.specificity(y_test, tvec_lr_preds), \n",
    "                          metrics.specificity(y_test, tvec_svc_preds),\n",
    "                          metrics.specificity(y_test, tvec_rf_preds), \n",
    "                          metrics.specificity(y_test, tvec_xgbc_preds)]\n",
    "\n",
    "tvec_sensitivity       = [recall_score(y_test, tvec_lr_preds), \n",
    "                          recall_score(y_test, tvec_svc_preds),\n",
    "                          recall_score(y_test, tvec_rf_preds), \n",
    "                          recall_score(y_test, tvec_xgbc_preds)]\n",
    "\n",
    "tvec_rocauc_score      = [roc_auc_score(y_test, tvec_lr_preds),\n",
    "                          roc_auc_score(y_test, tvec_svc_preds),\n",
    "                          roc_auc_score(y_test, tvec_rf_preds),\n",
    "                          roc_auc_score(y_test, tvec_xgbc_preds)]\n",
    "\n",
    "tvec_matthews_corrcoef = [matthews_corrcoef(y_test, tvec_lr_preds),\n",
    "                         matthews_corrcoef(y_test, tvec_svc_preds),\n",
    "                         matthews_corrcoef(y_test, tvec_rf_preds),\n",
    "                         matthews_corrcoef(y_test, tvec_xgbc_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lists into dataframes\n",
    "\n",
    "# A dataframe for the CVEC scores\n",
    "\n",
    "cvec_scores = pd.DataFrame(data    = [cvec_accuracy, cvec_specificity, \n",
    "                                      cvec_sensitivity, cvec_rocauc_score, \n",
    "                                      cvec_matthews_corrcoef],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Specificity\", \n",
    "                                      \"Sensitivity\", \"AUROC Score\", \n",
    "                                      \"Matthews Corr. Coef.\"])\n",
    "\n",
    "# A dataframe for the TVEC scores\n",
    "\n",
    "tvec_scores = pd.DataFrame(data    = [tvec_accuracy, tvec_specificity, \n",
    "                                      tvec_sensitivity, tvec_rocauc_score,\n",
    "                                      tvec_matthews_corrcoef],\n",
    "                           columns = [\"Log. Reg.\", \"SVC\", \"Random Forest\", \"XGBoost\"],\n",
    "                           index   = [\"Accuracy\", \"Specificity\", \n",
    "                                      \"Sensitivity\", \"ROC-AUC Score\",\n",
    "                                      \"Matthews Corr. Coef.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log. Reg.</th>\n",
       "      <th>SVC</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.942308</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC Score</th>\n",
       "      <td>0.733059</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.561813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.508376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.235864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\"><table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log. Reg.</th>\n",
       "      <th>SVC</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC-AUC Score</th>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.585623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.283772</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying the two dataframes side by side\n",
    "\n",
    "display_side_by_side(cvec_scores,\n",
    "                     tvec_scores)\n",
    "\n",
    "# The first table is the CVEC scores\n",
    "# The second table is the TVEC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cvec models\n",
    "\n",
    "cvec_predictions = pd.DataFrame([np.array(X_test),np.array(y_test), cvec_lr_preds, cvec_svc_preds, \n",
    "                                 cvec_rf_preds, cvec_xgbc_preds],\n",
    "                                index = [\"body\",\"Actual\",\"LgR.\", \"SVC\", \"RFC\", \"XGBC\"]).T\n",
    "\n",
    "# For tvec models\n",
    "\n",
    "tvec_predictions = pd.DataFrame([np.array(X_test),np.array(y_test), tvec_lr_preds, tvec_svc_preds, \n",
    "                                 tvec_rf_preds, tvec_xgbc_preds],\n",
    "                                index = [\"body\",\"Actual\",\"LgR.\", \"SVC\", \"RFC\", \"XGBC\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframes as csvs\n",
    "\n",
    "cvec_predictions.to_csv(\"./Reddit Data/All/cvec_model_predictions.csv\")\n",
    "tvec_predictions.to_csv(\"./Reddit Data/All/tvec_model_predctions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Ã¯Â»Â¿Author</th>\n",
       "      <th>body</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chestnutchaser</td>\n",
       "      <td>Hi, I was just wondering if anyone here has ha...</td>\n",
       "      <td>hi i wa just wondering if anyone here ha had a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casgemini</td>\n",
       "      <td>It worked as more of a last resort and worked ...</td>\n",
       "      <td>it worked a more of a last resort and worked w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saucy_pudding</td>\n",
       "      <td>You may want to see an internal medicine docto...</td>\n",
       "      <td>you may want to see an internal medicine docto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tortillalamp</td>\n",
       "      <td>I'm happy to hear that. Take care!</td>\n",
       "      <td>i m happy to hear that take care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alexx-gray</td>\n",
       "      <td>I’m the same, when I’m at home I hop in a cold...</td>\n",
       "      <td>i m the same when i m at home i hop in a cold ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ï»¿Ã¯Â»Â¿Author                                               body  \\\n",
       "0  chestnutchaser  Hi, I was just wondering if anyone here has ha...   \n",
       "1       casgemini  It worked as more of a last resort and worked ...   \n",
       "2   saucy_pudding  You may want to see an internal medicine docto...   \n",
       "3    tortillalamp                 I'm happy to hear that. Take care!   \n",
       "4      alexx-gray  I’m the same, when I’m at home I hop in a cold...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  hi i wa just wondering if anyone here ha had a...  \n",
       "1  it worked a more of a last resort and worked w...  \n",
       "2  you may want to see an internal medicine docto...  \n",
       "3                   i m happy to hear that take care  \n",
       "4  i m the same when i m at home i hop in a cold ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data= pd.read_csv('./Reddit Data/All/Reddit Eczema_All_20201201_Sample1.csv', encoding=\"iso-8859-1\" )\n",
    "\n",
    "test_data = cleaning_data(test_data)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataframe):\n",
    "\n",
    "    pred_text= dataframe[\"lemmatized_text\"]\n",
    "   \n",
    "    # For cvec models\n",
    "    cvec_lr_preds = cvec_lr_gs.predict(pred_text) \n",
    "    cvec_svc_preds = cvec_svc_gs.predict(pred_text) \n",
    "    cvec_rf_preds = cvec_rf_gs.predict(pred_text) \n",
    "    cvec_xgbc_preds = cvec_xgbc_gs.predict(pred_text) \n",
    "    \n",
    "    cvec_predictions = pd.DataFrame([np.array(pred_text),cvec_lr_preds, cvec_svc_preds, \n",
    "                                 cvec_rf_preds, cvec_xgbc_preds],\n",
    "                                index = [\"body\",\"LgR.\", \"SVC\", \"RFC\", \"XGBC\"]).T\n",
    "    \n",
    "    # For tvec models\n",
    "    tvec_lr_preds = tvec_lr_gs.predict(pred_text) \n",
    "    tvec_svc_preds = tvec_svc_gs.predict(pred_text) \n",
    "    tvec_rf_preds = tvec_rf_gs.predict(pred_text) \n",
    "    tvec_xgbc_preds = tvec_xgbc_gs.predict(pred_text) \n",
    "    \n",
    "    \n",
    "    tvec_predictions = pd.DataFrame([np.array(pred_text),tvec_lr_preds, tvec_svc_preds, \n",
    "                                 tvec_rf_preds, tvec_xgbc_preds],\n",
    "                                index = [\"body\",\"LgR.\", \"SVC\", \"RFC\", \"XGBC\"]).T\n",
    "    \n",
    "    # Saving the dataframes as csvs\n",
    "\n",
    "    cvec_predictions.to_csv(\"./Reddit Data/All/cvec_model_predictions1.csv\")\n",
    "    tvec_predictions.to_csv(\"./Reddit Data/All/tvec_model_predctions1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REF: https://www.kdnuggets.com/2019/09/reddit-post-classification.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
