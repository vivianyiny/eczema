{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Imports\n",
    "import nltk\n",
    "import pandas                        as pd\n",
    "import numpy                         as np\n",
    "import seaborn                       as sns\n",
    "import matplotlib.pyplot             as plt\n",
    "import re\n",
    "import ftfy\n",
    "from IPython.display                 import display_html\n",
    "from IPython.core.display            import display, HTML\n",
    "\n",
    "# Proprocessing, Modeling, & Evaluation\n",
    "from nltk.corpus                     import stopwords\n",
    "from nltk.stem                       import WordNetLemmatizer\n",
    "from nltk.tokenize                   import RegexpTokenizer \n",
    "from sklearn.ensemble                import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection         import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics                 import accuracy_score, recall_score, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.pipeline                import Pipeline\n",
    "from sklearn.svm                     import SVC\n",
    "#from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn                         import model_selection\n",
    "#from xgboost                         import XGBClassifier\n",
    "\n",
    "# Custom Modules\n",
    "import graphs\n",
    "import metrics\n",
    "\n",
    "\n",
    "# Notebook settings & styles\n",
    "sns.set(style = \"white\", palette = \"deep\")\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annotated_Sample = pd.read_csv('./Reddit Data/All/Reddit Eczema_All_20201201_Sample_Annotated.csv', encoding = \"iso-8859-1\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ying.Ying_Zenbook\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ying.Ying_Zenbook\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading the default stopwords\n",
    "\n",
    "nltk.download(\"stopwords\");\n",
    "\n",
    "# Adding our stopwords to the English set\n",
    "\n",
    "#new_stopwords = [\"like\", \"just\", \"make\", \"cook\",\"use\", \"chicken\", \"recipe\", \"sauce\"]\n",
    "\n",
    "stopwords     = stopwords.words('english')\n",
    "\n",
    "#stopwords.extend(new_stopwords)\n",
    "\n",
    "# Instantiating the lemmatizier and tokenizer\n",
    "# The tokenizer will only keep text\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer  = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(dataframe):\n",
    "   \n",
    "    if 'Annotation' in dataframe :\n",
    "        dataframe[\"label\"] = dataframe[\"Annotation\"].apply(lambda x: 1 if x == \"R\" else 0)\n",
    "    \n",
    "    #Annotated_Sample.drop(columns=['Annotation'],inplace=True)\n",
    "    dataframe.rename(columns={'Author':'author'},inplace=True)\n",
    "    \n",
    "    #lower case\n",
    "    \n",
    "    dataframe[\"body\"] = dataframe[\"body\"].astype(str)\n",
    "    \n",
    "    dataframe[\"body\"] = dataframe[\"body\"].str.lower()\n",
    "    \n",
    "    dataframe [\"body\"] = dataframe [\"body\"].map(ftfy.fix_encoding)\n",
    "    \n",
    "    #remove URL\n",
    "    dataframe[\"body\"] = dataframe[\"body\"].str.replace(\"http\\S+\", \"\")\n",
    "    \n",
    "    #remove none letters\n",
    "    \n",
    "    dataframe[\"body\"] =  dataframe[\"body\"].apply(lambda x: re.sub(r'[^a-z]',' ',str(x)))\n",
    "    \n",
    "    # Setting up the lemmatizer\n",
    "\n",
    "    lemmatized_posts = []\n",
    "\n",
    "    for post in dataframe[\"body\"]:\n",
    "        tokens = tokenizer.tokenize(post)\n",
    "        post   = [lemmatizer.lemmatize(post) for post in tokens]\n",
    "        lemmatized_posts.append(\" \".join(post))\n",
    "\n",
    "    # Appending the lemmatized posts to the dataframe\n",
    "\n",
    "    dataframe[\"lemmatized_text\"] = lemmatized_posts\n",
    "\n",
    "    # Checking the head of the dataframe\n",
    "    #dataframe.head()\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>label</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>touchyfeelies</td>\n",
       "      <td>thanks</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Replacementheart</td>\n",
       "      <td>oh wow  i m local and i just moved here haha</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>oh wow i m local and i just moved here haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpy_Frenchman</td>\n",
       "      <td>but if you never stop</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>but if you never stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnhingedMom</td>\n",
       "      <td>google calendar  same place i track the rest o...</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>google calendar same place i track the rest of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>princesskailynkitten</td>\n",
       "      <td>wow   very thorough  thank you so much for inc...</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>wow very thorough thank you so much for includ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                               body  \\\n",
       "0         touchyfeelies                                            thanks    \n",
       "1      Replacementheart       oh wow  i m local and i just moved here haha   \n",
       "2      Grumpy_Frenchman                           but if you never stop      \n",
       "3           UnhingedMom  google calendar  same place i track the rest o...   \n",
       "4  princesskailynkitten  wow   very thorough  thank you so much for inc...   \n",
       "\n",
       "  Annotation  label                                    lemmatized_text  \n",
       "0          I      0                                             thanks  \n",
       "1          I      0        oh wow i m local and i just moved here haha  \n",
       "2          I      0                              but if you never stop  \n",
       "3          I      0  google calendar same place i track the rest of...  \n",
       "4          I      0  wow very thorough thank you so much for includ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Annotated_Sample = cleaning_data(Annotated_Sample)\n",
    "Annotated_Sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the          1780\n",
       "it           1743\n",
       "and          1688\n",
       "to           1585\n",
       "my           1144\n",
       "             ... \n",
       "the first      23\n",
       "bed            23\n",
       "now and        23\n",
       "not sure       23\n",
       "good luck      23\n",
       "Length: 500, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_lem = CountVectorizer(ngram_range =(1,2),stop_words = None,min_df=10,max_features=500)\n",
    "\n",
    "vec_sample_lem     = vec_lem.fit_transform(Annotated_Sample[\"lemmatized_text\"])\n",
    "\n",
    "sample_vectorized_lem     = pd.DataFrame(vec_sample_lem.toarray(), columns = vec_lem.get_feature_names())\n",
    "\n",
    "sample_vectorized_lem.sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency) was invented for document search and information retrieval. It works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. \n",
    "\n",
    "\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thanks       1.0\n",
       "your skin    0.0\n",
       "having       0.0\n",
       "got          0.0\n",
       "great        0.0\n",
       "            ... \n",
       "seem         0.0\n",
       "seems        0.0\n",
       "severe       0.0\n",
       "shampoo      0.0\n",
       "able         0.0\n",
       "Name: 0, Length: 500, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec_lem = TfidfVectorizer(ngram_range =(1,2),stop_words = None,min_df=10,max_features=500)\n",
    "\n",
    "tvec_sample_lem     = tvec_lem.fit_transform(Annotated_Sample[\"lemmatized_text\"])\n",
    "\n",
    "tsample_vectorized_lem     = pd.DataFrame(tvec_sample_lem.toarray(),columns = tvec_lem.get_feature_names())\n",
    "\n",
    "tsample_vectorized_lem.T[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "__Create Test and Training data split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = Annotated_Sample[\"lemmatized_text\"]\n",
    "y = Annotated_Sample[\"label\"]\n",
    "\n",
    "# The random state ensures reproducability\n",
    "# The stratify argument preserves the distribution of classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify     = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "A support vector machine (in this case a classifier) is at its core a linear model. However, instead of running like a logistic regression, it seeks to linearly separate the data. To do that, it uses a kernel to raise the data into n-dimensional space. It then uses a line, plane (3-dimensional line), or hyperplane (greater than 3-dimensions) to delineate the data\n",
    "\n",
    "__Count Vectorizer__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cvec__max_features': 250, 'cvec__min_df': 2, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'svc__C': 1.5, 'svc__gamma': 0.35, 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "cvec_svc_pipe = Pipeline([(\"cvec\", CountVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting CVEC and pipe hyperparameters\n",
    "\n",
    "cvec_pipe_params = {\"cvec__max_features\": [250], \n",
    "                    \"cvec__ngram_range\" : [(1,2)], \n",
    "                    \"cvec__stop_words\"  : [stopwords],\n",
    "                    \"cvec__min_df\"      : [2],\n",
    "                    \"svc__C\"            : [1,1.5,2],\n",
    "                    \"svc__kernel\"       : [\"linear\",\"rbf\"],\n",
    "                    \"svc__gamma\"        : [0.15,0.25, 0.35]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "cvec_svc_gs = GridSearchCV(cvec_svc_pipe, \n",
    "                           param_grid = cvec_pipe_params, \n",
    "                           cv         = 10,\n",
    "                           refit=True)\n",
    "\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "cvec_svc_gs.fit(X_train, y_train);\n",
    "\n",
    "# print best parameter after tuning \n",
    "print(cvec_svc_gs.best_params_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generating training predictions\n",
    "\n",
    "cvec_svc_train_preds = cvec_svc_gs.predict(X_train)\n",
    "\n",
    "# Generating test predictions\n",
    "\n",
    "cvec_svc_preds       = cvec_svc_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.983153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.980966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.993243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.987105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.945306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.983153\n",
       "Sensitivity           0.980966\n",
       "Specificity           0.993243\n",
       "AUROC                 0.987105\n",
       "Matthews Corr. Coef.  0.945306"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, cvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7638802245789145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.888489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.956332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.763880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.585403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.888489\n",
       "Sensitivity           0.956332\n",
       "Specificity           0.571429\n",
       "AUROC                 0.763880\n",
       "Matthews Corr. Coef.  0.585403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "print(roc_auc_score(y_test, cvec_svc_preds))\n",
    "\n",
    "metrics.binary_classification_summary(y_test, cvec_svc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>10</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                   28                 21\n",
       "Actual Related                     10                219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   cvec_svc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF Vectorizer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svc__C': 2, 'svc__gamma': 1, 'svc__kernel': 'rbf', 'tvec__max_features': 250, 'tvec__min_df': 2, 'tvec__ngram_range': (1, 2), 'tvec__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]}\n"
     ]
    }
   ],
   "source": [
    "# Setting up the pipeline\n",
    "\n",
    "tvec_svc_pipe = Pipeline([(\"tvec\", TfidfVectorizer()), \n",
    "                         (\"svc\", SVC())])\n",
    "\n",
    "# Setting TFIDF and pipe hyperparameters\n",
    "\n",
    "tvec_pipe_params = {\"tvec__max_features\": [250], \n",
    "                    \"tvec__ngram_range\" : [(1,2)], \n",
    "                    \"tvec__stop_words\"  : [stopwords],\n",
    "                    \"tvec__min_df\"      : [2],\n",
    "                    \"svc__C\"            : [2,2.5,3.5],\n",
    "                    \"svc__kernel\"       : [\"linear\",\"rbf\"],\n",
    "                    \"svc__gamma\"        : [0.5,0.75,1]}\n",
    "                    \n",
    "# Instantiating the grid search\n",
    "\n",
    "tvec_svc_gs = GridSearchCV(tvec_svc_pipe, \n",
    "                           param_grid = tvec_pipe_params, \n",
    "                           cv         = 10)\n",
    "\n",
    "\n",
    "# Fitting the model to the training data\n",
    "\n",
    "tvec_svc_gs.fit(X_train, y_train);\n",
    "\n",
    "# print best parameter after tuning \n",
    "print(tvec_svc_gs.best_params_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training predictions\n",
    "\n",
    "tvec_svc_train_preds = tvec_svc_gs.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test predictions\n",
    "\n",
    "tvec_svc_preds       = tvec_svc_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.984356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.985359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.979730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.982544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.947914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.984356\n",
       "Sensitivity           0.985359\n",
       "Specificity           0.979730\n",
       "AUROC                 0.982544\n",
       "Matthews Corr. Coef.  0.947914"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training metrics\n",
    "\n",
    "metrics.binary_classification_summary(y_train, tvec_svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.874101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.707023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.508484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Score\n",
       "Accuracy              0.874101\n",
       "Sensitivity           0.965066\n",
       "Specificity           0.448980\n",
       "AUROC                 0.707023\n",
       "Matthews Corr. Coef.  0.508484"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test metrics\n",
    "metrics.binary_classification_summary(y_test, tvec_svc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Irrelated</th>\n",
       "      <th>Predicted Related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Irrelated</th>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Related</th>\n",
       "      <td>8</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted Irrelated  Predicted Related\n",
       "Actual Irrelated                   22                 27\n",
       "Actual Related                      8                221"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a confusion matrix on the test results\n",
    "\n",
    "metrics.confusion_matrix_dataframe(y_test, \n",
    "                                   tvec_svc_preds,\n",
    "                                   columns = [\"Predicted Irrelated\", \"Predicted Related\"],\n",
    "                                   index   = [\"Actual Irrelated\", \"Actual Related\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows for dataframes to be displayed side-by-side\n",
    "\n",
    "def display_side_by_side(*args):\n",
    "    html_str = ''\n",
    "    for df in args:\n",
    "        html_str += df.to_html()\n",
    "    display_html(html_str.replace('table', 'table style=\"display:inline\"'), raw = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vec_accuracy          = [\n",
    "                          accuracy_score(y_test, cvec_svc_preds),\n",
    "                          accuracy_score(y_test, tvec_svc_preds)\n",
    "                        ]\n",
    "\n",
    "vec_specificity       = [\n",
    "                          metrics.specificity(y_test, cvec_svc_preds),\n",
    "                          metrics.specificity(y_test, tvec_svc_preds)\n",
    "                        ]\n",
    "\n",
    "vec_sensitivity       = [\n",
    "                          recall_score(y_test, cvec_svc_preds),\n",
    "                          recall_score(y_test, tvec_svc_preds)\n",
    "                        ]\n",
    "\n",
    "vec_rocauc_score      = [\n",
    "                          roc_auc_score(y_test, cvec_svc_preds),\n",
    "                          roc_auc_score(y_test, tvec_svc_preds)\n",
    "                        ]\n",
    "\n",
    "vec_matthews_corrcoef = [\n",
    "                         matthews_corrcoef(y_test, cvec_svc_preds),\n",
    "                        matthews_corrcoef(y_test, tvec_svc_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lists into dataframes\n",
    "\n",
    "# A dataframe for the CVEC scores\n",
    "\n",
    "vec_scores = pd.DataFrame(data    = [vec_accuracy, vec_specificity, \n",
    "                                      vec_sensitivity, vec_rocauc_score, \n",
    "                                      vec_matthews_corrcoef],\n",
    "                           columns = [ \"CVEC\",\"TVEC\"],\n",
    "                           index   = [\"Accuracy\", \"Specificity\", \n",
    "                                      \"Sensitivity\", \"AUROC Score\", \n",
    "                                      \"Matthews Corr. Coef.\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"display:inline\" border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVEC</th>\n",
       "      <th>TVEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.874101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Specificity</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <td>0.956332</td>\n",
       "      <td>0.965066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC Score</th>\n",
       "      <td>0.763880</td>\n",
       "      <td>0.707023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matthews Corr. Coef.</th>\n",
       "      <td>0.585403</td>\n",
       "      <td>0.508484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table style=\"display:inline\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying the two dataframes side by side\n",
    "\n",
    "display_side_by_side(vec_scores)\n",
    "\n",
    "# The first table is the CVEC scores\n",
    "# The second table is the TVEC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_predictions = pd.DataFrame([np.array(X_test),np.array(y_test), cvec_svc_preds, \n",
    "                                 tvec_svc_preds],\n",
    "                                index = [\"body\",\"Actual\",\"CVEC.\",\"TVEC\"]).T\n",
    "vec_predictions.to_csv(\"./Reddit Data/All/cvec_model_predictions_tuning.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cvec_svc_gs_file = 'cvec_svc_gs.pkl'\n",
    "pickle.dump(cvec_svc_gs, open(cvec_svc_gs_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_svc_gs_file = 'tvec_svc_gs.pkl'\n",
    "pickle.dump(tvec_svc_gs, open(tvec_svc_gs_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models from disk\n",
    "cvec_svc_gs_load = pickle.load(open(cvec_svc_gs_file, 'rb'))\n",
    "tvec_svc_gs_load = pickle.load(open(tvec_svc_gs_file, 'rb'))\n",
    "\n",
    "#load_cvec_svc_gs = pickle.load(cvec_svc_gs)\n",
    "#load_tvec_svc_gs = pickle.load(tvec_svc_gs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataframe):\n",
    "\n",
    "    pred_text= dataframe[\"lemmatized_text\"]\n",
    "    \n",
    "  \n",
    "    # For cvec models\n",
    "   \n",
    "    cvec_svc_preds_test = cvec_svc_gs_load.predict(pred_text) \n",
    "\n",
    "    # For tvec models\n",
    "    tvec_svc_preds_test = tvec_svc_gs_load.predict(pred_text) \n",
    "    \n",
    "    # For tvec models\n",
    "    vec_svc_preds_test = cvec_svc_preds_test+tvec_svc_preds_test\n",
    "       \n",
    "    vec_predictions = pd.DataFrame([np.array(pred_text),cvec_svc_preds_test, tvec_svc_preds_test,vec_svc_preds_test],index = [\"body\",\"cvec\", \"tvec\",\"all\"]).T\n",
    "    \n",
    "    # Saving the dataframes as csvs\n",
    "    vec_predictions.to_csv(\"./Reddit Data/All/Reddit Eczema_All_20201201_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Author</th>\n",
       "      <th>body</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaka_x</td>\n",
       "      <td>definitely  it s heavenly</td>\n",
       "      <td>definitely it s heavenly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comicsandpoppunk</td>\n",
       "      <td>t gel  does the trick for me</td>\n",
       "      <td>t gel doe the trick for me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dezstern</td>\n",
       "      <td>ayayayay  i have been where you are  i really ...</td>\n",
       "      <td>ayayayay i have been where you are i really ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dezstern</td>\n",
       "      <td>i            ve tried a lot  lot  of various t...</td>\n",
       "      <td>i ve tried a lot lot of various thing this is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dezstern</td>\n",
       "      <td>huh  will try that</td>\n",
       "      <td>huh will try that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ï»¿Author                                               body  \\\n",
       "0            Vaka_x                        definitely  it s heavenly     \n",
       "1  comicsandpoppunk                      t gel  does the trick for me    \n",
       "2          dezstern  ayayayay  i have been where you are  i really ...   \n",
       "3          dezstern  i            ve tried a lot  lot  of various t...   \n",
       "4          dezstern                                huh  will try that    \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0                           definitely it s heavenly  \n",
       "1                         t gel doe the trick for me  \n",
       "2  ayayayay i have been where you are i really ho...  \n",
       "3  i ve tried a lot lot of various thing this is ...  \n",
       "4                                  huh will try that  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data= pd.read_csv('./Reddit Data/All/Reddit Eczema_All_20201201.csv', encoding=\"iso-8859-1\" )\n",
    "\n",
    "test_data = cleaning_data(test_data)\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REF: https://www.kdnuggets.com/2019/09/reddit-post-classification.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
